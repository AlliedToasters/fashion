{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.5/dist-packages (0.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.7.2)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.14.2)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.5/dist-packages (from pandas) (2018.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2->pandas) (1.11.0)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.5/dist-packages (2.1.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras) (3.12)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras) (1.14.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (2.7.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from h5py) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.5/dist-packages (from h5py) (1.14.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.5/dist-packages (5.1.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.5/dist-packages (0.19.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.5/dist-packages (2.2.2)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (1.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (2.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (2.7.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.5/dist-packages (from matplotlib) (2018.4)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (1.14.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from kiwisolver>=1.0.1->matplotlib) (39.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.5/dist-packages (2.18.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests) (2018.4.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests) (1.22)\n",
      "mkdir: cannot create directory 'test_imgs': File exists\n",
      "mkdir: cannot create directory 'train_imgs': File exists\n",
      "mkdir: cannot create directory 'val_imgs': File exists\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install keras\n",
    "!pip install h5py\n",
    "!pip install pillow\n",
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install requests\n",
    "\n",
    "!mkdir test_imgs train_imgs val_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import sys\n",
    "from capsulenet import CapsNet, margin_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_labels(lbls):\n",
    "    \"\"\"takes a string and returns list of numeric labels\"\"\"\n",
    "    result = lbls[2:][:-2]\n",
    "    result = result.split(\"', '\")\n",
    "    return result\n",
    "\n",
    "#!unzip train_imgs.csv.zip\n",
    "df = pd.read_csv('train_imgs.csv').copy()\n",
    "df.index = df.img_id.astype(int)\n",
    "\n",
    "def update_progress(progress):\n",
    "    \"\"\"Displays or updates a console progress bar\n",
    "    Accepts a float between 0 and 1. Any int will be converted to a float.\n",
    "    A value under 0 represents a 'halt'.\n",
    "    A value at 1 or bigger represents 100%\n",
    "    \"\"\"\n",
    "    barLength = 25 # Modify this to change the length of the progress bar\n",
    "    status = \"\"\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "        status = \"error: progress var must be float\\r\\n\"\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "        status = \"Halt...\\r\\n\"\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "        status = \"Done...\\r\\n\"\n",
    "    block = int(round(barLength*progress))\n",
    "    text = \"\\rProgress: [{0}] {1}% {2}\".format( \"#\"*block + \"-\"*(barLength-block), round(progress*100, 2), status)\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def features_to_sparse(df, filename='sparse_features.npz'):\n",
    "    \"\"\"Converts labels to sparse matrix. Tries loading\n",
    "    file; if fails, does all the work and saves it (takes a bit.)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = load_npz(filename)\n",
    "        return result\n",
    "    except:\n",
    "        print('file not found. Creating...')\n",
    "    total_rows = len(df)\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for idx in df.index:\n",
    "        progress = idx/total_rows\n",
    "        update_progress(progress)\n",
    "        df_row = df.loc[idx]\n",
    "        labels = parse_labels(df_row.lbls)\n",
    "        for lbl in labels:\n",
    "            row.append(idx-1)\n",
    "            col.append(int(lbl)-1)\n",
    "            data.append(1)\n",
    "            \n",
    "    row, col, data = np.array(row), np.array(col), np.array(data)\n",
    "    result = csr_matrix((data, (row, col)), shape=(max(row)+1, max(col)+1))\n",
    "    save_npz(filename, result)\n",
    "    return result\n",
    "\n",
    "mat = features_to_sparse(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = mat.sum(axis=0)\n",
    "totals = [totals[0, i] for i in range(228)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.83% of instances belong to top 10 classes.\n",
      "The top 10 classes are: \n",
      "[ 62  53 106 222  66 214  17 171 153 105]\n"
     ]
    }
   ],
   "source": [
    "n_classes = 10\n",
    "totals = np.array(totals)\n",
    "top_n = np.argpartition(totals, -n_classes)[-n_classes:]\n",
    "check = totals[top_n].copy()\n",
    "topn_frac = check.sum()/totals.sum()\n",
    "print('{}% of instances belong to top {} classes.'.format(round(topn_frac*100, 2), n_classes))\n",
    "print('The top {} classes are: '.format(n_classes))\n",
    "#add one because of zero indexing\n",
    "print(top_n + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(9, 6));\n",
    "plt.bar(x=range(1,229), height=totals);\n",
    "plt.title('appearance frequency by class');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trunc_matrix = mat[:, top_n]\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(np.array(trunc_matrix.sum(axis=1))[:, 0], bins=7);\n",
    "plt.title('Top {} class co-occurence'.format(n_classes));\n",
    "plt.xlabel('number of top {} items in photo'.format(n_classes));\n",
    "plt.ylabel('number number of photos');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018174667633932093"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(np.array(trunc_matrix.sum(axis=1))[:, 0]==0))/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    \"\"\"takes a url and returns a PIL image object.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "        print('request failed.')\n",
    "        return\n",
    "    if response.status_code in [403, 404]:\n",
    "        print('image not found.')\n",
    "        return\n",
    "    try:\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    except:\n",
    "        print('could not open image.')\n",
    "        return\n",
    "    return img\n",
    "\n",
    "def image_from_url(idx, df=df):\n",
    "    \"\"\"uses dataframe and index to lookup url\n",
    "    and returns PIL image.\"\"\"\n",
    "    row = df.loc[idx]\n",
    "    url = row['url']\n",
    "    image = download_image(url)\n",
    "    return image\n",
    "\n",
    "def image_from_disk(idx, df=df, path_to_images='./train_imgs/'):\n",
    "    \"\"\"Loads an image from disk according to index idx.\n",
    "    returns PIL image.\n",
    "    \"\"\"\n",
    "    file = path_to_images + str(idx) + '.png'\n",
    "    image = Image.open(file)\n",
    "    return image\n",
    "\n",
    "def image_as_tensor(image, xy_dims=(256, 256)):\n",
    "    \"\"\"takes a PIL image (image) and returns\n",
    "    a numpy array with shape xy_dims (color channels\n",
    "    last) scaled between 0 and 1.\n",
    "    \"\"\"\n",
    "    res = image.resize(xy_dims)\n",
    "    res = np.array(res)/255\n",
    "    return res\n",
    "\n",
    "def get_image(idx, filepath='./train_imgs/', df=df, xy_dims=(256, 256)):\n",
    "    \"\"\"attempts to load image from disk. If not found,\n",
    "    downloads and saves to disk.\"\"\"\n",
    "    try:\n",
    "        img = image_from_disk(idx, path_to_images=filepath)\n",
    "        return image_as_tensor(img)\n",
    "    except:\n",
    "        img = image_from_url(idx, df=df)\n",
    "        img = img.resize(xy_dims)\n",
    "        img.save(filepath + str(idx) + '.png')\n",
    "        return image_as_tensor(img)\n",
    "    \n",
    "def input_batch(indices, filepath='./train_imgs/', df=df, xy_dims=(256, 256)):\n",
    "    \"\"\"Creates an input batch with given list of indices.\"\"\"\n",
    "    batch = []\n",
    "    for idx in indices:\n",
    "        batch.append(get_image(idx, filepath=filepath, df=df, xy_dims=xy_dims))\n",
    "    batch = np.array(batch)\n",
    "    return batch\n",
    "    \n",
    "def output_batch(indices, matrix=trunc_matrix, n_placeholders=2):\n",
    "    \"\"\"Creates an output batch with given list of indices.\"\"\"\n",
    "    batch = []\n",
    "    for idx in indices:\n",
    "        label_array = trunc_matrix[idx,:].toarray()[0,:]\n",
    "        to_append = np.ones(n_placeholders)\n",
    "        final_array = np.append(label_array, to_append)\n",
    "        batch.append(final_array)\n",
    "    batch = np.array(batch)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_ideal_images(df=df, model_path='ideal_classidier.h5', batch_size=10):\n",
    "    \"\"\"Applies classifier model to images in df and returns a prediction vector\n",
    "    of length len(df)\"\"\"\n",
    "    from keras.models import load_model\n",
    "    model = load_model(model_path)\n",
    "    result = np.array([])\n",
    "    batch_begin = 1\n",
    "    batch_end = batch_begin + batch_size\n",
    "    while batch_begin < max(df.index):\n",
    "        progress = batch_begin/len(df)\n",
    "        update_progress(progress)\n",
    "        indices = [i for i in range(batch_begin, batch_end)]\n",
    "        batch_begin += batch_size\n",
    "        batch_end += min(batch_size, len(df)-batch_end)\n",
    "        batch = input_batch(indices)\n",
    "        prediction = model.predict(batch)\n",
    "        result = np.append(result, prediction)\n",
    "        np.save('ideal_image_classes.npy', result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_predictions = np.empty(len(df))\n",
    "ideal_predictions[:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen(\n",
    "    batch_size=10,\n",
    "    df=df, \n",
    "    shuffle=False, \n",
    "    ideal_predictions=ideal_predictions,\n",
    "    shit_img=shit_img):\n",
    "    \"\"\"Generates training batches.\"\"\"\n",
    "    current_image = 0\n",
    "    while True:\n",
    "        batch_indices = []\n",
    "        while len(batch_indices) < batch_size:\n",
    "            current_image += 1\n",
    "            if current_image > len(df):\n",
    "                current_image = 1\n",
    "            if shuffle:\n",
    "                current_image = np.random.randint(1, len(df))\n",
    "            if ideal_predictions[current_image-1]>.5:\n",
    "                batch_indices.append(current_image)\n",
    "            elif ideal_predictions[current_image-1]<.5:\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception('problem classifying image as ideal.')\n",
    "        X = input_batch(batch_indices)\n",
    "        Y = output_batch(batch_indices)\n",
    "        yield [X, Y], [Y, X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv('validation.csv')\n",
    "val_data.index = val_data.img_id.astype(int)\n",
    "val_mat = features_to_sparse(val_data, 'validation_matrix.npz')\n",
    "trunc_val = val_mat[:, top_n]\n",
    "\n",
    "\n",
    "def val_gen(batch_size=10, df=val_data, mat=trunc_val, filepath='./val_imgs/'):\n",
    "    \"\"\"Generates validation batches.\"\"\"\n",
    "    current_image = 0\n",
    "    while True:\n",
    "        batch_indices = []\n",
    "        while len(batch_indices) < batch_size:\n",
    "            current_image += 1\n",
    "            if current_image > len(df):\n",
    "                current_image = 1\n",
    "            batch_indices.append(current_image)\n",
    "        X = input_batch(batch_indices, filepath=filepath, df=df)\n",
    "        Y = output_batch(batch_indices, matrix=mat)\n",
    "        yield [X, Y], [Y, X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/fashion/capsulelayers.py:137: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 248, 248, 256 62464       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)      (None, 120, 120, 256 5308672     conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)    (None, 460800, 8)    0           primarycap_conv2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)      (None, 460800, 8)    0           primarycap_reshape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "digitcaps (CapsuleLayer)        (None, 12, 16)       707788800   primarycap_squash[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 12)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mask_1 (Mask)                   (None, 192)          0           digitcaps[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "capsnet (Length)                (None, 12)           0           digitcaps[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Sequential)            (None, 256, 256, 3)  202147328   mask_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 915,307,264\n",
      "Trainable params: 915,307,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "capsnet, eval_model, manipulate_model = CapsNet((256, 256, 3), 12, 3)\n",
    "capsnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "capsnet.compile(optimizer='adam',\n",
    "              loss=[margin_loss, 'mse'],\n",
    "              loss_weights=[1., 0.392],\n",
    "              metrics={'capsnet': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"input_2:0\", shape=(?, 12), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1075\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1076\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3589\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3668\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3669\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3670\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"input_2:0\", shape=(?, 12), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-58eeca38051d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3299\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1076\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1078\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"input_2:0\", shape=(?, 12), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "#tg = train_gen()\n",
    "#vg = val_gen(batch_size=3)\n",
    "history = capsnet.fit_generator(\n",
    "    tg,\n",
    "    steps_per_epoch=1000,\n",
    "    validation_data=vg,\n",
    "    validation_steps=3299\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy, yx = next(tg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = xy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"input_1:0\", shape=(?, 256, 256, 3), dtype=float32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m             subfeed_t = self.graph.as_graph_element(\n\u001b[0;32m-> 1075\u001b[0;31m                 subfeed, allow_tensor=True, allow_operation=False)\n\u001b[0m\u001b[1;32m   1076\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3589\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3668\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3669\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3670\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"input_1:0\", shape=(?, 256, 256, 3), dtype=float32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-670ad3548195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcapsnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1835\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1076\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m             raise TypeError(\n\u001b[0;32m-> 1078\u001b[0;31m                 'Cannot interpret feed_dict key as Tensor: ' + e.args[0])\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"input_1:0\", shape=(?, 256, 256, 3), dtype=float32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "capsnet.predict(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 256, 256, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
